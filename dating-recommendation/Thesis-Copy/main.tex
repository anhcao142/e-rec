%\documentclass[12pt, oneside]{report}
\documentclass[oneside,13pt]{extreport}
%\KOMAoption{fontsize}{16pt}
%\documentclass[13pt]{extarticle}  
\usepackage[ a4paper, total={155mm,232mm}, left=35mm, top=30mm,]{geometry}

\usepackage[utf8]{inputenc}
%\usepackage{mathptmx} % Times new Roman font
%\usepackage[charter]{mathdesign}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[lined, linesnumbered, longend]{algorithm2e}
\usepackage{listings}
\usepackage{nameref}
\usepackage{url}
\usepackage{imakeidx}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage[labelsep=newline,font=sc, justification=centerlast]{caption}
\graphicspath{ {figures/} }
\usepackage{array}
\makeindex[columns=2, intoc]
\usepackage{setspace}
\usepackage[english]{babel} % for vietnamese support
\usepackage{tikz} % for drawing rectangle
\usepackage{color, colortbl}
%\usepackage[]{algorithm2e}
\usetikzlibrary{calc}
\usepackage{breqn}


%\newenvironment{mysce}
%  {\begin{quote}\itshape}
%  {\end{quote}}

\lstdefinestyle{mystyle}{
    basicstyle={\footnotesize},
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,
    columns=fullflexible,
    aboveskip=1mm,
    belowskip=1mm,
    numbers=none,
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4
}

\lstset{style=mystyle}

\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}%
\newcommand{\B}[1]{\textbf{#1}}  %Bold
\newcommand{\I}[1]{\emph{#1}}    %Italics
\newcommand{\ws}[1]{\url{#1}}
\newcommand*\cleartoleftpage{%
  \clearpage
  \ifodd\value{page}\hbox{}\newpage\fi
}
\renewcommand{\baselinestretch}{1.5}
\definecolor{Gray}{gray}{0.7}
\newcolumntype{g}{>{\columncolor{Gray}}c}

\begin{document}

\clearpage
%\input{titlepage}
\chapter*{Acknowledgements}
\renewcommand{\thepage}{\roman{page}}
\setcounter{page}{1}

\addcontentsline{toc}{chapter}{Acknowledgements}
First and foremost, we have to thank our thesis supervisors, Mr. Mai Le Tung and Mr Le Ngoc Thanh. Without their assistance and dedicated involvement in every step throughout the process, this thesis would have never been accomplished. We would like to thank you very much for your support and understanding over these past four years.

We would also like to show gratitude to our committee, including Prof. Vu DUONG, Assoc.Prof. Minh-Triet TRAN, Dr. Ba-Tien DINH, Quoc-Hoang VU and Dr. Thai-Son TRAN and all of our lecturers and teaching assistants in all four academic year at University of Science. Dr. Ba-Tien DINH was our first-year lecturer at HCM University of Science. His teaching style and enthusiasm for the topic made a strong impression on us and we have always carried positive memories of his classes with us.

Getting through our thesis required more than academic support, and we have many, many people to thank for supporting and, at times, having to tolerate us over the past year. We cannot begin to express our gratitude and appreciation for their friendship. Viet-Hoang NGUYEN QUANG, Phong-Quoc DANG, and Tri NGUYEN have been unwavering in their personal and professional support during the time we spent at the University. For many memorable evenings out and in, we must thank everyone above as well as Quang-Minh Trinh, Phuc-Quang LE and Quang-Thang TAT. We would also like to thank Hue-Dang Nguyen who helped us find data on the internet and lent his house when we need a place to work.

Most importantly, none of this could have happened without our family. To our parents, our brothers and sisters – who always be besides to encourage us with their love and support. Every time we was ready to quit, they did not let us and we are forever grateful. This thesis stands as a testament to their unconditional love and encouragement.

Finally, we would like thank to each other - a strong passionate research team - for spending, sharing spirit, patience, idea, algorithm, methods, time, space, result, resource, sad and happiness through many difficulties of the thesis together.


\renewcommand\contentsname{Table of contents}
\addcontentsline{toc}{chapter}{Table of contents}
\tableofcontents
\clearpage

\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures
 \clearpage
 
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables
 

\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}
Recommender systems were developed to help users deal with information. These systems have become an important part of e-commerce. With the tremendous growth of e-commerce, scalability is one of the biggest challenges for recommender systems. 
To address the scalability problem, we propose \textit{Incremental SVD++} method which enables online integration of new ratings. \textit{Incremental SVD++} is based on the state-of-the-art recommendation algorithm SVD++. To our knowledge, our work is the first to extend SVD++ with incremental updates. Our method improves performance of classic SVD++, while maintaining the recommendation quality.

\chapter{Introduction}
\renewcommand{\thepage}{\arabic{page}}
\setcounter{page}{1}
\section{Recommender systems}
\index{Recommender systems}
The explosive growth  of the Internet and electronic commerce gives users many benefits. The penetration of the Internet and technology into all areas of human activity has further boosted the volume of online information resources. Every single day in this information era, we create 2.5 quintillion bytes of data, mean about 2.5 billion gigabytes, according to the Big Data research of IBM\footnote{http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html}. The amount of information available on the Internet has become immense and is still increasing exponentially. On one hand, the abundance of online information may guarantee that users are able to find what they are looking for. On the other hand, this abundance also makes the useful information difficult to find. Users are facing an explosion of choices. The availability of choices, instead of producing a benefit, started to reduce the comfort of users. It is understood that while the choice is good, more choice is not always better. Every day, users have difficulty deciding which movie to watch, which book to read, which course to take and where to go for a travel. Therefore, information overload became a big challenge. 

To deal with this challenge, Recommender systems were born. Since the appearance of the first research in the mid-1990s~\cite{Hill, Resnick, Shardanand}, recommender systems have become an important research area. These systems attempt to suggest information/items that a user may be interested in. The suggestions provided are aimed at helping users to cope with the information and choice over-abundance. 

\subsection{Applications of recommender systems}
Recommender systems have become one of the most powerful and popular tools in electronic commerce. These systems have been successfully applied and played a pivot role in many online services.  Internet Movie Database (IMDb)\footnote{http://www.imdb.com/} and Movie-Lens\footnote{https://movielens.org/} recommend movies to users. Amazon\footnote{http://www.amazon.com/} and Taobao\footnote{http://www.taobao.com} have personalized recommendations based on a user’s past purchase history, rating data and review data. Recommender systems also have been  in many dating services such as LibimSeTi\footnote{http://www.libimseti.cz/} and Perfect Match\footnote{http://www.perfectmatch.com/}.

Figure~\ref{fig:amazon} shows a screen shot of recommendations on Amazon.
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"amazon".png} 
    \caption{Recommendations of Amazon}
    \label{fig:amazon}
\end{figure}
%\clearpage


With these applications, E-commerce websites (e.g. Amazon, CDNow\footnote{http://CDNow.com/}) can build customer loyalty, increase profits and boost item-cross selling. In fact, it is reported that 35\% product sales of Amazon come from recommendations, 38\% more click-throughs on Google news are generated by recommendations, and over two thirds of the rented movies of Netflix\footnote{http://www.netflix.com/} are recommended\cite{Chevalier}.


\subsection{Types of recommender systems}
According to how recommendations are made, recommender systems are usually classified into 3 types\cite{Adomavicius, Balabanovic}: \emph{content-based filtering}, \emph{collaborative filtering}, and \emph{hybrid recommender systems}. Fig. 2 shows the anatomy of different recommendation techniques.
\clearpage
\begin{figure}[h!]
\centering
\includegraphics[width=\textwidth]{"RS techniques".pdf}
\caption{Recommendation techniques}
\label{fig:RS_techniques}
\end{figure}

\begin{description}
\index{Content-based filtering}
\index{Collaborative filtering}
\index{Hybrid recommender}
    \item{\textbf{Content-based filtering}} recommends an item to a user by matching up the features of the item with the preferences of the user. The item features and the user's preferences are learnt by analyzing the profiles. For example, a movie profile might describe its genre, the participating actors, directors. User profiles could include demographic information such as gender, age, hobbies. The features of the content as well as the preferences of the user have to be learnt. This is usually very hard because the inherent diversity of contents of items. Content-based filtering has been successfully applied in areas such as news recommendation and music recommendation.
    \item{\textbf{Collaborative filtering}} relies only on user past behaviours (previous transactions or product ratings). In addition, relying directly on user behavior allows uncovering complex and unexpected patterns that would be difficult or impossible to profile using known data attributes\cite{BellKorFactor}. There are two main approaches of collaborative filtering: memory-based approach and model-based approach. Memory-based approaches are focused on computing the relationships between users or items. Model-based approaches process observed ratings to build a predefined compact model in the training phase which is then used to make predictions.
    \item{\textbf{Hybrid recommender}} combines multiple recommendation techniques together to eliminate the limitations of pure approaches\cite{TranHybrib}.
\end{description}


\section{Problem statement}
Despite significant growth in the past decades, recommender systems still face many challenging problems. One of the biggest problems is the trade-off between accurate estimation prediction and the time required to calculate them.  To address this problem, we need to consider two following goals simultaneously.

The first goal is that how to improve the quality of the recommendations for the customers. The accuracy of recommendations is the first criteria to evaluate a recommender system. For example, a recommender system recommends a list of products for a user, Joe. But Joe finds out that he does not like the suggested products, so Joe will be unlikely to use that recommender system again. He does not care about how fast the recommendations are produced or how diversity the recommendations are.

The Second goal is how to improve the scalability of the recommender systems. The tremendous growth of customers and products in recent years poses two key challenges for recommender systems: large-scale data and not being scalable to the demands of real world applications. For example, a recommender system recommends a list of products for a user, Joe. He likes all the suggested products, but he must wait thirty minutes to get the list of recommendations. Joe will be unlikely to use that recommender system again. 

These are conflicts goals which require trade-offs. If an algorithm spends less time for estimating prediction, it will be more scalable but produce worse quality. 

Many researchers suggest that Singular Value Decomposition-based (SVD-based) methods may be a solution to improve the quality of recommender systems. SVD-based methods produced results that were better than a traditional CF algorithm most of the time when applied to the MovieLens data set \cite{SarwarApplication}. 

A new approach of SVD was presented by Simon Funk in the context of the Netflix Prize\cite{SimonFunk}. This method used an approximate way to compute the low-rank approximation of the matrix by minimizing the squared error loss. Since then, many other SVD-based models have been created. One of them is SVD++ model, which is desribed by Yehuda Koren, the winner of Netflix Prize. SVD++ model relies on both explicit feedback and implicit feedback\cite{BellKorFactor}. Implicit feedback is an especially valuable information source for users who do not provide much explicit feedback. It indirectly reflect opinion through observing user behavior. Hence, SVD++ uses implicit feedback as a secondary source of information. This model improves prediction accuracy to some extent.

Despite the high accuracy of recommendations, the matrix factorization step of SVD-based approaches is computationally very expensive because it takes a lot of memory and time to factorize a matrix. This limitations make SVD-based approach less suitable for large scale deployment in e-commerce system. 

In order to deal with this, many e-commerce systems prefer to compute the model offline and feed the database with updated information periodically~\cite{Linden}. These systems provide recommendations to users quickly, based on pre-computed models. However, without considering the data submitted between two offline computations, these recommendations  are not produced with the highest possible degree of confidence. 

Let's look at a scenario: 
\begin{quote}
A young man, Harry loved the classical music. Every day, he listened to music on ABC online music website. this website recommended to him many classical songs. One day, he heard a rock songs and very loved it. He wanted to listen more rock music. He opened ABC website. Of course, it still recommended him classical songs. So he searched it by himself. After listening many songs on ABC website, he reloaded the page to get new recommendations but it was full of classical songs. And in the day after, ABC website suggested him a list of classical songs. What happened to the recommendation of ABC website?
\end{quote}
The problem described in the scenario is happened with many recommender systems in real-world applications. 
\section{Thesis Contribution}
In this thesis, we present studies that improve recommender systems by solving the above mentioned problems. We propose \emph{Incremental SVD++} method which extends SVD++ with incremental updates. Our method suitable for the dynamic scenario faced by recommender systems today. New users and new items join the system constantly. Our method significant improves scalability of classic SVD++, while maintaining its recommendation quality. The pre-computed model is updated incrementally at the time of rating activity and recommended items are modified based on newest data. Experimental results have shown a significant increase in training speed without a loss in accuracy.

The rest of the paper is organized as follows.
\begin{description}
    \item{\textbf{Chapter~\ref{backgroud_chapter}}} reviews the background knowledge of recommender systems.More specifically, we present content-based filtering, collaborative filtering and hybrid recommender. We focus more on collaborative filtering, which is the dominating method in recommender systems nowadays.
    \item{\textbf{Chapter~\ref{ISVD++_chapter}}} introduces several matrix factorization techniques like SVD and SVD++. We also propose a method incremental SVD++. 
	\item{\textbf{Chapter~\ref{experiments}}} presents our experimental procedure and results of the proposed methods on the MovieLens dataset. 
    \item{\textbf{Chapter~\ref{Summarization}}} is the short summarization of Incremantal SVD++ and the developments of a simple application of Incremantal SVD++.
    
\end{description}

\chapter{Background}
\label{backgroud_chapter}
\section{Recommender systems}
\index{Recommender systems}
Recommender systems are a subclass of information filtering system that collects users’ preferences over time and try to make predictions on which items that a user might like in the future. Put simply, the aim of recommender systems is predicting ratings for the items that a user has not seen before. Once we can estimate a user's ratings for all unrated items, we can recommend the items which received the highest predicted value.

Recommender systems can broadly be classified into two types: content-based filtering and collaborative filtering. Both techniques have their advantages and disadvantages. Hybrid approaches, combining collaborative filtering and content-based filtering could be more effective in some cases. They can eliminate the limitations of pure techniques.

Figure~\ref{fig:CF vs CbF} shows the principle of theses two techniques.
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"CF vs CbF".png} 
    \caption{The principle behind collaborative and content-based filtering}
    \!\!\!\!
    \caption*{Source: The Marketing Technologist.}
    \label{fig:CF vs CbF}
\end{figure}

\section{Content-based filtering}
\index{Content-based filtering}
\index{CbF}
In content-based filtering (CbF) approaches, a user is suggested items similar to those he liked or preferred in the past. Content-based filtering techniques  make recommendations by comparing a user profile with the features of each item. Each user has a profile which describes his preferences.  

For example, a user Joe has watched and rated the following movies: 
\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Movies & Green Lantern & American Pie & Hangover & Saw & ...   \\
        \hline
        Rating & 8 & 9 & 10 & 4 & ...\\
        \hline
    \end{tabular}
    \caption*{List of watched movies}
\end{table}

The list of movies and their attribute-values:
\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
         & Action & Adventure & Children & Comedy & Crime & Drama   \\
        \hline
        Green Lantern & 1 & 1 & 0 & 1 & 0 & 0 \\
        \hline
        American Pie & 0 & 0 & 0 & 1 & 0 & 1 \\
        \hline
        Hangover & 0 & 1 & 0 & 1 & 0 & 1 \\
        \hline
        Saw & 1 & 0 & 0 & 0 & 1 & 0 \\
        \hline
        Home Alone & 1 & 1 & 1 & 1 & 0 & 1 \\
        \hline
        ... & ... & ... & ... & ... & ... & ... \\
        \hline
    \end{tabular}
    \caption*{Attribute-values of movies}
\end{table}

The build of a user’s profile does not depend on other users’ behavior. The features of the items rated by a user are assumed to reflect the user's preferences. So content-based recommendation algorithms build a user's profile based on these features. Therefore, profile of Joe are built based on attribute-values of rated movies, i.e., Green Lantern, American Pie, Hangover,Saw, etc.

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
         & Action & Adventure & Children & Comedy & Crime & Drama   \\
        \hline
       	Joe & 0.6 & 0.5 & -0.3 & 0.2 & 0.2 & -0.4 \\
        \hline
    \end{tabular}
    \caption*{Profile of Joe}
\end{table}


Figure~\ref{fig:CbF Architecture} shows the high level architecture of a content-based recommender. There are three major components, each component handles a step in the recommendation process~\cite{Lops11}.
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"CbF Architecture".png} 
    \caption{High level architecture of a Content-based Recommender~\cite{Lops11}}
    \!\!\!\!
    \label{fig:CbF Architecture}
\end{figure}


\begin{description}
    \item{\textbf{Content Analyzer}} performs pre-processing step that transforms the raw content of an item (e.g. documents, Web pages, news, product descriptions, etc.) into the form suitable for the next processing steps, like feature representation. These represented items are the input to the Profile Learner and Filtering Component
    \item{\textbf{Profile Learner}} collects data related to the past preferences of a user and tries to construct the user profile. The user profile could be a prototypical item feature vector that represents the user’s preferences.
    \item{\textbf{Filtering Component}} computes relevance scores between items  and given a user, based on the items’ features and user profile. Then this component produces the list of recommendations based on these scores.
\end{description}    
    In this process, feedback of the active user is collected in order to construct and update the user's profile. Content analyzer step is very important. With the suitable item representation, content-based filtering method produces high accuracy recommendations. Content analyzer usually involves domain dependent experts who select and devise the
features suitable for the description of the items. For example, the features used for music recommendation are very different from the features used for news article recommendation. In addition, the content analyzer usually requires domain knowledge.

An advantage of content-based filtering is that explanations on how the recommender system works can be provided. Explanation on why a particular item is recommended can help user decide whether to take further actions. 

Another advantage is that content-based filtering can recommend items with no previous ratings. With this advantage, content-based filtering has seen wide application in news recommendation which has the distinct feature of extremely large set of items. the items to recommend are breaking news that have no or little previous data. To recommend a news article content-based filtering analyzes the content of the article and match it with the preferences of a user, without the need to collect users’ ratings on this article. On the other hand, content-based filtering is unable to handle the new users which has no or few preferences available. With no or few previous preferences, we cannot construct a reliable user profile, a key component in successful recommendation. So content-based filtering solves half of the cold-start problem.

The biggest disadvantage of content-based filtering is inability at discovering new unexpected items. The system does not recommend these items that are different from anything that the user has seen before. Sometimes this might become problem
because the user might want to try something new.


\section{Collaborative filtering}
\index{Collaborative filtering}
Collaborative filtering(CF) became one of most successful recommender technique since this approach was mentioned and described by Paul Resnick and Hal Varian in 1997~\cite{Resnick}. The underlying opinion of the CF approaches is that users who shared preferences in the past tend to share similar preferences in the future. With this opinion,
we assume that there is a low-rank structure of the user item rating
matrix and mathematical techniques can be used to fill this matrix
and thus make recommendations.
 
Figure~\ref{fig:CF process} shows the schematic diagram of the collaborative filtering process. There are two main tasks:
\begin{description}
    \item{\textbf{Predict task}} computes ratings that the target user gives to unrated items. 
    \item{\textbf{Recommend task}} produces the best ranked list of n items for the target user’s need.
\end{description}
%\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"CF process".png} 
    \caption{The Collaborative Filtering Process.}
    \label{fig:CF process}
\end{figure}
\clearpage    
Collaborative filtering models rely only on the past user behavior (i.e. previous transactions and ratings) without requiring the creation of explicit profiles. 

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & Green Lantern & American Pie & Hangover & Saw   \\
        \hline \hline
        $u_1$ & 5 & 3 &  & 1 \\
        \hline
        $u_2$ & 4 &  &  & 1 \\
        \hline
        $u_3$ & 1 & 1 &  & 5 \\
        \hline
        $u_4$ & 1 &  &  & 4 \\
        \hline
        $u_5$ &  & 1 & 5 & 4 \\
        \hline
    \end{tabular}
    \caption{Sample ratings matrix (on a 5-star scale)}
    \label{tab:rating_matrix-1}
\end{table}

Typically, the data processed by a CF-based recommender system can be illustrated as in Table~\ref{tab:rating_matrix-1}. This is a rating matrix of 5 users and 4 items. Each row represents a distinct user and each column represents one item, which is a movie here. Each cell contains the rating of the movie from 1 to 5 as provided by the users; 1 being the lowest rating and 5 being the highest one. Since all the users haven't seen or rated all of the movies, some cells remain vacant because of lack of information. 

CF approaches take the ratings of the target user on the seen items and use them to predict the ratings for this user for the unseen items. Then, items are recommended in a descending order according to their predicted ratings.

Collaborative filtering techniques can be divided into two categories: memory-based and model-based. Although simple and easy to implement, memory-based methods have the limitations in prediction accuracy and time consuming to make a prediction. 

Despite the success of CF techniques, these techniques suffer from various problems such as Cold-start problem of new-user/new-item, data sparsity problem, and scalability.

In the following of this section,we first describe the notations that will be used throughout this thesis. We then review two approaches
of collaborative filtering, including memory-based methods and model-based methods.

\subsection{Notations}
The notations used in this thesis are described as follows. Suppose that we are given
a set of $M$ users $U = \left\{ {{u_1},{u_2},...,{u_M}} \right\}$ and a set of $N$ items $I = \left\{ {{i_1},{i_2},...,{i_M}} \right\}$. Users’ rating on the items are arranged in a $N \times M$ matrix $R$ where entry $r_{u,i}$ denote the rating that user $u$ gives to item
$i$ and $r_{u,i} = 0$ if $u$ have not rated $i$. The set of all items that user $u$
have rated is denoted by $I_u$ and the set of all users who have rated item $i$ is denoted by $U_i$. Alternatively, we denote the set of all observed
triplet $\left( {u,i,r} \right) \in K$  where $u$ is the user id, $i$ is the item id
and $r$ is the rating given by $u$ to $i$. The whole set is denoted by $K$.
Other notations used which are model specific will be discussed in the context. 

Rating oriented collaborative filtering tries to produce prediction for the rating that is likely to be assigned to $i$ by $u$, i.e. $r_{u,i}$. We denote the prediction by $\hat r_{u,i}$. Instead of giving prediction of ratings, ranking oriented collaborative
filtering output a rank $\pi$ of the items in decreasing order
of preference.
\subsection{Memory-based methods}
\index{Memory-based methods}
The idea of memory-based methods (also called neighborhood-based) is that the rating predictions for a user directly depend on the ratings, on that item, of similar users (i.e. neighbors).  Approaches are further divided into two techniques, user-based and item-based.

\subsubsection{Similarity measures}
\index{Pearson correlation}
\index{Cosine similarity}
One of the most important factors that affect the result of memory-based methods for collaborative filtering is how the similarity between
users or items is measured. Note that the similarity measure is between two vectors. Let $s(u,v)$ denote the similarity measure between $u$ and $v$. Several different similarity functions have been proposed and evaluated in the literature.

\begin{description}
    \item{\textbf{Pearson correlation}} 
    \\Pearson correlation coefficient is a similarity measure between two vectors. In essence, it measures the correlation between two vectors with respect to the product of their standard deviation. The correlation is computed by the following:
    \begin{eqnarray}
	\label{eq:pearson_sim}
	s\left( {u,v} \right) = \frac{{\sum\nolimits_{i \in {I_u} \cap {I_v}} {({r_{u,i}} - \bar r_u )({r_{v,i}} - \bar r_v )} }}{{\sqrt {\sum\nolimits_{i \in {I_u} \cap {I_v}} {{{({r_{u,i}} - \bar r_u )}^2}} } \sqrt {\sum\nolimits_{i \in {I_u} \cap {I_v}} {{{({r_{v,i}} - \bar r_v )}^2}} } }}
	\end{eqnarray} 
    \item{\textbf{Cosine similarity}} 
    \\It measures the cosine value of the angle between two vectors in high dimensional space. Its definition is given in equation~\ref{eq:cosine_sim}
    \begin{eqnarray}
	\label{eq:cosine_sim}
	s\left( {u,v} \right) = \frac{{{r_u}.{r_v}}}{{\left\| {{r_u}} \right\|.\left\| {{r_v}} \right\|}} = \frac{{\sum\nolimits_i {{r_{u,i}}{r_{v,i}}} }}{{\sqrt {\sum\nolimits_i {{r^2}_{u,i}} } \sqrt {\sum\nolimits_i {{r^2}_{vi}} } }}
	\end{eqnarray}
	
\end{description}
  
\subsubsection{User-based methods}
\index{User-based methods}
In user-based methods, the prediction of an item for the target user is based on his/her similar users’ ratings on it. The unknown ratings $\hat r_{u,i}$ is predicted using
a set of other users’ rating for item $i$. Let $s\left( {v,u} \right)$ denotes the similarity
measure between 2 users $u$ and $v$. $N_u$ is the set of $K$ neighbors of user $u$ and $U_i$ is all the users who have rated $i$. The prediction can be calculated as:
\begin{eqnarray}
\label{eq:user_based_pre1}
\hat r_{u,i} = \frac{{\sum\limits_{v \in {N_u} \cap {U_i}} {{s\left( {u,v} \right)}.{r_{v,i}}} }}{{\sum\limits_{v \in {N_u} \cap {U_i}} {\left| {s\left( {u,v} \right)} \right|} }}
\end{eqnarray}

Equation 2.1 gives a simple method to calculate the prediction. However, there are several issues associated with this method. The first problem is that users may have very different rating behaviors. Some users will tend to give higher or lower ratings than others. A conservative user might never give more than 3 while an optimism user would rate the items on the scale from 3 to 5. That is, the ratings given by a user could be biased. It is easy to see that this simple method could give rating prediction wildly deviate from the true rating. As a simple solution for this is to adjust a user’s ratings with his mean rating~\cite{Resnick94}. This would give a slightly modified prediction:

\begin{eqnarray}
\label{eq:user_based_pre2}
\hat r_{u,i} = \bar r_u+\frac{{\sum\limits_{v \in {N_u} \cap {U_i}} {{s\left( {u,v} \right)}.({r_{v,i}}-\bar r_v)} }}{{\sum\limits_{v \in {N_u} \cap {U_i}} {\left| {s\left( {u,v} \right)} \right|}}}
\end{eqnarray}
In equation~\ref{eq:user_based_pre2}, $\bar r_u$ denotes the mean ratings given by $u$:
\begin{eqnarray}
\label{eq:mean_rating}
\bar r_u = \frac{{\sum\nolimits_{i \in {I_u}} {{r_{u,i}}} }}{{\left| {{I_u}} \right|}}
\end{eqnarray}

Several different approaches have been proposed to alleviate the bias problem by normalizing the rating before calculating the similarity.

The time complexity of user-based approaches is $O(N^2xMxK)$ for the neighborhood model construction and $O(K)$ for the rating prediction. The  space complexity is O(NxK)`\cite{Chevalier}.

\subsubsection{Item-based methods}
\index{Item-based methods}
Item-based methods are very similar with user-based methods. The prediction of an item for a user is based on the user’s ratings on its similar items (neighbors). The assumption is that users would assign similar
scores for similar items. Let $s\left( {i,j} \right)$ denotes the similarity measure between 2 item $i$ and $j$. $N_i$ is the set of $K$ neighbors of item $i$ and $I_u$ is all the items that user $u$ has rated. The prediction of item $i$ for user $u$ can be calculated as:

\begin{eqnarray}
\label{eq:item_based_pre}
\hat r_{u,i} = \bar r_i+\frac{{\sum\limits_{j \in {N_i} \cap {I_u}} {{s\left( {i,j} \right)}.({r_{u,j}}-\bar r_j)} }}{{\sum\limits_{j \in {N_i} \cap {I_u}}  {\left| {s\left( {i,j} \right)} \right|} }}
\end{eqnarray}

In equation~\ref{eq:item_based_pre}, $\bar r_i$ denotes the mean ratings on item $i$:

\begin{eqnarray}
\label{eq:mean_rating_item}
\bar r_i = \frac{{\sum\nolimits_{u \in {U_i}} {{r_{u,i}}} }}{{\left| {{U_i}} \right|}}
\end{eqnarray}

Item-based methods could produce more accurate prediction and efficient than user based-methods~\cite{Sarwar01}.

The time complexity of item-based approaches is $O(M^2xNxK)$ for the neighborhood model construction and $O(K)$ for the rating prediction. The  space complexity is O(MxK)~\cite{Chevalier}.
\subsection{Model-based methods}
\index{Model-based methods}
Model-based approaches provide a systematic way to train a predefined compact model in the training phase that explains observed ratings, which is then used to make predictions. The general idea is to build a model offline and use that model for online rating. For example, the system can build cluster model to divide users into $k$ groups based on similarities. Then in order to determine the community of a user, we need to solve a classification problem. These approaches potentially offers the benefits of both speed and scalability.

The model building process is performed by different machine learning algorithms such as Bayesian network, clustering, and rule-based approaches. The state-of-the-art model-based methods include restricted
Boltzmann machines~\cite{Salakhutdinov07}, SVD++~\cite{KorenMatrix}, Probabilistic Matrix Factorization (PMF)~\cite{SalakhutdinovMF07}, and multi-domain collaborative filtering~\cite{Zhang10}, graphical models~\cite{Jin03}, pair-wise tensor factorization~\cite{Rendle10}, and matrix factorization with social regularization~\cite{Ma11}, etc.

There are several advantages of model-based collaborative filtering methods:
\begin{description}
    \item{\textbf{Scalability:}} In model-based algorithms, precomputed models are much smaller than the actual dataset. So the model is used efficiently even for very large datasets. This improves scalability of the systems. 
    \item{\textbf{Prediction speed:}} Model-based collaborative filtering methods can quickly produce the recommendation for they use pre-computed model. This advantage is very importance to commercial applications such as online E-commerce websites.
\end{description}

Despite these advantages, model-based methods have a limitation. Building a model is often a time- and resource-consuming process, it is executed periodically. Therefore, it is difficult to add new data to systems, making them inflexible.

\section{Hybrid recommendation approaches}
\index{Hybrid recommendation approaches}
Content-based and collaborative filtering approaches have been widely used in commercial and research areas, but they still have many limitations. Therefore, the hybrid approach has been introduced to avoid the limitations of pure approaches. Hybrid recommender systems combine multiple techniques of collaborative approaches and contentbased approaches together to produce its output. Hybrid approaches are classified into seven different types~\cite{Burke07}:

\begin{description}
    \item{\textbf{Weighted:}}  the score of a item is a combination of the scores provided by different recommendation components  using a linear combination or a voting scheme. 
    \item{\textbf{Switching:}} this is a special case of the weighted type considering binary weights. The system chooses only one recommendation technique among the others and applies it. 
    \item{\textbf{Mixed:}} recommendations from several components are available, and are presented together at the same time by means of certain ranking or combination strategy.
    \item{\textbf{Feature combination:}} the features used by different recommenders are integrated into a single data source, which is exploited by a single recommender.
    \item{\textbf{Feature augmentation:}} the output of a recommender is used as an additional input feature for the next recommender.
    \item{\textbf{Cascade:}} the recommendation is performed as a sequential process. The next recommender refines the recommendations given by the previous one. 
    \item{\textbf{Meta-level:}} the model generated by a recommender is used as the input for the next one. As stated in~\cite{Burke02}: “this differs from feature augmentation: in an augmentation hybrid, we use a learned model to generate features for input to a second algorithm; in a meta-level hybrid, the entire model becomes the input.”
\end{description}

\chapter{Incremental SVD++}
\label{ISVD++_chapter}

\section{Singular Value Decomposition based models}
\index{Matrix Factorization}
\index{Latent factor models}
Before delving into models, we first describe the notations used in this chapter. Suppose that we are given
a set of $M$ users $U = \left\{ {{u_1},{u_2},...,{u_N}} \right\}$ and a set of $N$ items $I = \left\{ {{i_1},{i_2},...,{i_M}} \right\}$. Users’ rating on the items are arranged in a $M \times N$ matrix $R$ where entry $r_{u,i}$ denote the rating that user $u$ gives to item $i$ and $r_{u,i} = 0$ if $u$ have not rated $i$. The predicted value of $r_{u,i}$ is denoted by $\hat r_{u,i}$. The set of all items that user $u$
have rated is denoted by $I_u$ and the set of all users who have rated item $i$ is denoted by $U_i$. Alternatively, we denote the set of all observed
triplet $\left( {u,i,r} \right) \in K$  where $u$ is the user id, $i$ is the item id
and $r$ is the rating given by $u$ to $i$. The whole set is denoted by $K$. 

\subsection{Fundamentals of Matrix factorization}

In recent years, many SVM algorithms such as SVM, boosting, ... was exploited by researchers to solve recommendation problems. One of the most popular algorithms is latent factor models. The underlining assumption of latent factor model is that we can find some hidden factors which could link users and items. And that helps to explain the ratings. 

%\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"latent_factors_example".jpg} 
    \caption{A simplified illustration of the latent factor approach, which
characterizes both users and movies using two axes—male versus female
and serious versus escapist.}
\caption*{Source: Netflix Prize diagram~\cite(KorenMatrix)}
    \label{fig:LFM_example}
\end{figure}

Some of the most successful realizations of latent factor models are based on matrix factorization. Matrix factorization based methods use low-rank matrix to approximate user item rating matrix $R$. They learn two low-rank matrices, one captures users’ latent features - $P$ and the other capture items’ latent features- $Q^T$. 

%\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"Matrix factorization".png} 
    \caption{A illustration of the Matrix factorization}
    \label{fig:MF}
\end{figure}

The product of $P$ and $Q^T$ approximately equals to $R$. 
\begin{eqnarray}
\label{eq:MF_predict}
R \approx P\times{Q^T} = \widehat R
\end{eqnarray}
 The approximation is to minimize the error between the predicted and the original rating matrix.
  
\subsubsection{Matrix factorization techniques}
\index{stochastic gradient descent}
There are two standard approaches to learn $P$ and $Q^T$: stochastic
gradient descent(SGD) and alternating least squares (ALS).
\begin{description}
    \item{\textbf{Stochastic gradient descent(SGD)}} 
    \\Stochastic gradient descent (SGD) is widely used in many machine learning problems. SGD try to find minimums or maximums by iteration. In an SGD (Stochastic Gradient descent) approach, for each example in the training set the error ${e_{u,i}} \stackrel{\text{def}}{=} \;{r_{u,i}} - \hat r_{u,i}$ is computed and the parameters ($p_u$ and $q_i$) are updated by a factor in the opposite direction of the gradient.

$\begin{array}{l}{p_u} \leftarrow {p_u} + {\rm{\;}}\gamma \left( {{e_{u,i}}{q_i} - \lambda {p_u}} \right)\\{q_i} \leftarrow {q_i} + {\rm{\;}}\gamma \left( {{e_{u,i}}{p_u} - \lambda {q_i}} \right)\end{array}
$    
    
    \item{\textbf{Alternating Least Squares(ALS)}}
    \\ALS represents a different approach to optimizing the loss function. The values in $P$ are fixed when solving $Q^T$ and vice versa. This allows for an optimal solving as it makes the
optimization problem quadratic. 
\end{description}

In this section we describe several matrix factorization techniques like SVD and SVD++.

\subsection{Baseline predictors}
 Some users will tend to give higher or lower ratings than others. And some items received higher or lower ratings than others. So we consider two parameter: one parameter $b_u$ for each user and one $b_i$ for each item. The parameters $b_u$ and $b_i$ indicate the observed deviations of user $u$ and item $i$ from the average. For example, the average rating over all movies is 3.7 stars. Furthermore, \emph{Titanic} is better than an average movie, so it receives 0.5 stars above the average. On the other hand, \emph{Joe} is a critical user, who tends to rate 0.3 stars lower than the average. Thus, 0.5 and -0.3 reflects the deviations of \emph{Titanic} and \emph{Joe} from the global average 0.37. 
 
The overall average rating is denoted by $\mu$. Prediction is done by the rule:
\begin{eqnarray}
\label{eq:Baseline_predict}
{\hat r_{u,i}} = \mu + {b_u} + {b_i}
\end{eqnarray}

The parameters $b_u$ and $b_i$ are estimated by solving the least squares problem: 
\begin{eqnarray}
mi{n_{{b_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in K} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i}} \right)^2} + \;\lambda \left( {{b_u}^2 + \;{b_i}^2} \right)
\label{eq:Baseline_minimize}
{\hat r_{u,i}} = \mu + {b_u} + {b_i}
\end{eqnarray}

 A simple gradient descent technique was applied successfully to solving (\ref{eq: Baseline_minimize}).
We loop through all ratings in the training set. The associated prediction error is denoted by ${e_{u,i}} \stackrel{\text{def}}{=} \;{r_{u,i}} - \hat r_{u,i}$. For a given training case $r_{u,i}$, we modify the parameters by moving in the opposite direction of the gradient, yielding:
$
\begin{array}{l}{b_u} \leftarrow {b_u} + {\rm{\;}}\gamma .\left( {{e_{u,i}} - {\rm{\;}}\lambda .{b_u}} \right)\\{b_i} \leftarrow {b_i} + {\rm{\;}}\gamma .\left( {{e_{u,i}} - {\rm{\;}}\lambda .{b_i}} \right)\end{array}
$
\\where $\gamma$ is the learning rate.
\subsection{Regularized Singular Value Decomposition (Regularized-SVD)}
\index{Singular Value Decomposition}
\index{SVD}
Singular value decomposition (SVD) is one type of matrix factorization. Matrix factorization models transform both users and items into the same latent feature space of dimensionality $f$. The prediction is done by taking an inner product in that space: 
\begin{eqnarray}
\label{eq:MF_predict}
{\hat r_{u,i}} = q_i^T{p_u} 
\end{eqnarray}

where each item $i$ is associated with a vector ${q_i} \in {\mathbb{R}^f}$ , and each user $u$ is associated with a vector ${p_u} \in {\mathbb{R}^f}$. For a given item i, the elements of $q_i$
measure the extent to which the item possesses those factors; for a given user $u$, the elements of $p_u$ measure the extent of interest the user has in items that are high on the corresponding factors. Therefore, their dot product $q_i^T{p_u}$ denotes the overall interest of the user in characteristics of the item.

For example, we apply matrix factorization methods on computing the rating that user $u_2$ gives to item $Apollo 13$. 

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & Titanic & Mummy & Apollo 13 & Spider Man \\
        \hline
        $u_1$ & 5 & 3 &  & 1 \\
        \hline
        $u_2$ & 4 & 1 & \cellcolor{Gray}? & 1 \\
        \hline
        $u_3$ & 1 &  &  & 5 \\
        \hline
        $u_4$ & 1 & 1 &  & 4 \\
        \hline
        $u_5$ &  &  & 5 & 4 \\
        \hline
    \end{tabular}
    \caption*{Ratings matrix $R(m \times n)$, $m=5$, $n=4$}
\end{table}

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|}
        \hline
         & $f_1$ & $f_2$ \\
        \hline
        $u_1$ & 2.44 & 0.23\\
        \hline
        \rowcolor{Gray}
        $u_2$ & 1.96 & 0.27  \\
        \hline
        $u_3$ & 0.46 & 2.16  \\
        \hline
        $u_4$ & 0.46 & 1.75 \\
        \hline
        $u_5$ & 0.46 & 1.77 \\
        \hline
    \end{tabular}
    \caption*{Users features matrix $P_{mxk}$, $k=2$}
\end{table}
\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|g|c|c|}
        \hline
         & Titanic & Mummy & Apollo 13 & Spider Man \\
        \hline
        $f_1$ & 1.95 & 1.14 & 1.54 & 0.22 \\
        \hline
        $f_2$ & 0.06 & 0.18 & 2.17 & 2.13 \\
        \hline
    \end{tabular}
    \caption*{Items features matrix ${Q^T}_{m \times n}$, $k=2$}
\end{table}

Therefore, the rating that user $u_2$ gives to item $Apollo 13$ is:

$\hat r_{u2,Apollo 13} = 1.96$ x $1.54 + 0.27$ x $2.17 = 3.6$

We learn the values of the factor vectors (${p_u}$ and ${q_i}$) by minimizing the regularized squared error function associated with (\ref{eq:MF_predict})

\begin{equation}
mi{n_{{q_*},{p_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in K} {\left( {{r_{u,i}} - \;\;q_i^T{p_u}} \right)^2} + \;\lambda \left( {\;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2}} \right)
\label{eq: MBMF}
\end{equation}

$K$ is the set of the (u, i) pairs for which $r_{u,i}$ is known. 

The first term ${\left( {{r_{u,i}} - \;\;q_i^T{p_u}} \right)^2}$ strives to find $q_i$'s, $p_u$'s that fit the given ratings.
The regularizing term – $\lambda \left( {\;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2}}  \right)$ – avoids overfitting by penalizing the magnitudes of the parameters.

    A simple gradient descent technique was applied successfully to solving (\ref{eq: MBMF}).
We loop through all ratings in the training set. The associated prediction error is denoted by ${e_{u,i}} \stackrel{\text{def}}{=} \;{r_{u,i}} - \hat r_{u,i}$. For a given training case $r_{u,i}$, we modify the parameters by moving in the opposite direction of the gradient, yielding:

$\begin{array}{l}{q_i} \leftarrow {q_i} + {\rm{\;}}\gamma \left( {{e_{u,i}}{p_u} - {\lambda}{q_i}} \right)\\{p_u} \leftarrow {p_u} + {\rm{\;}}\gamma \left( {{e_{u,i}}{q_i} - {\lambda}{p_u}} \right)\end{array}
$

where $\gamma$ is the learning rate.
    


\subsection{SVD with Bias Terms(Bias-SVD )}
We add biases to the regularized SVD model. Therefore, in this model,
a rating is predicted by the rule:

\begin{equation}
{\hat r_{u,i}} = \;\mu  + \;{b_u} + \;{b_i} + q_i^T{p_u}
\label{eq: SVD}
\end{equation}

To learn the model parameters, we should also minimize the regularized squared error:
\begin{equation}
mi{n_{{q_*},{p_*},\;\;{b_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in K} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i} + \;q_i^T\;{p_u}} \right)^2} + \;\lambda \left( {{b_u}^2 + \;{b_i}^2 + \;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2}} \right)
\end{equation}

Similarly, we use the gradient descent method to get the update rule for each parameter:

$
\begin{array}{l}{b_u} \leftarrow {b_u} + {\rm{\;}}\gamma .\left( {{e_{u,i}} - {\rm{\;}}{\lambda _1}.{b_u}} \right)\\{b_i} \leftarrow {b_i} + {\rm{\;}}\gamma .\left( {{e_{u,i}} - {\rm{\;}}{\lambda _1}.{b_i}} \right)\\{p_u} \leftarrow {p_u} + {\rm{\;}}\gamma .\left( {{e_{u,i}}.{q_i} - {\lambda _2}.{p_u}} \right)\\{q_i} \leftarrow {q_i} + {\rm{\;}}\gamma .\left( {{e_{u,i}}.{p_u} - {\lambda _2}.{q_i}} \right)\end{array}
$
\\
where ${e_{u,i}} \stackrel{\text{def}}{=} \;{r_{u,i}} - \hat r_{u,i}$ and $\gamma$ is the learning rate.

\subsection{SVD with implicit feedback (SVD++)}
\subsubsection{Explicit and implicit feedback}
Recommender systems rely on various types of input such as explicit feedback and implicit feedback. 

Explicit feedback includes explicit input by users regarding their interest in items. For example, Amazon collects star ratings for products and TiVo users indicate their
preferences for TV shows by hitting thumbs-up/down buttons. It is difficult to obtain explicit  feedback from a population of users because it requires effort from the users and also, users are not always ready to supply enough information. 

 On the other hand, implicit feedback is abundant. Implicit feedback indirectly reflect opinion through observing user behaviors~\cite{Oard98}. Types of implicit feedback include purchase history, browsing history, search patterns, or even mouse movements. For
example, a user who purchased many books by the same author probably likes that
author or  a user listens to a track 5 times may has an interest in that track.

Explicit feedback is generally more accurate than implicit feedback in representing the user’s interests. Also, explicit feedback can be positive or negative, whereas implicit feedback is only positive. A combination of explicit feedback and implicit feedback can minimize their weaknesses and perform a better recommendation.

\subsubsection{SVD with implicit feedback (SVD++)}
SVD++ is another extension of SVD method. SVD++ uses implicit feedback as a secondary source of information. It also combines bias terms into the system. So this model produces more accurate prediction than other factor models. 

For simplicity, we consider the rated history of users as a part of input data. This type of implicit feedback can be extracted from ratings matrix by reducing the ratings matrix into a binary matrix. $p_{u,i}$, which indicates the preference of user $u$ to item $i$, are derived by binarizing the $r_{u,i}$ values:

\begin{eqnarray}
\label{eq:implicit_binary}
q_{u,i} = \begin{cases} 1 & \quad \text{if } r_{u,i} > 0\\ -0 & \quad \text{if } r_{u,i} = 0\\ \end{cases}
\end{eqnarray}

The set of all items for which $u$ provided an implicit preference is denoted by $N_u$. To keep generality, each user $u$ is associated with two sets of items $I_u$ and $N_u$.

To this end, a second set of item factors is added, relating each item $i$ to a factor
vector ${y_i} \in {\mathbb{R}^f}$ . Those new item factors are used to characterize users based on the
set of items that they rated. The exact model is as follows:
\begin{eqnarray}
\label{eq:svd++_pre}
{\hat r_{u,i}} = \;\mu  + \;{b_i} + \;{b_i} + q_i^T\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in  {N_u}} {{y_j}} } \right)
\end{eqnarray}

Now, a user $u$ is modeled as $p_u + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in  {N_u}} {{y_j}}$ . We use a free user-factors
vector, $p_u$, much like in (\ref{eq: SVD}), which is learnt from the given explicit ratings. This
vector is complemented by the sum ${{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in {N_u}} {{y_j}}$, which represents the perspective
of implicit feedback. Since the $y_j$’s are centered around zero (by the regularization),
the sum is normalized by ${{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}$, in order to stabilize its variance
across the range of observed values of $N_u$.

As usual, we learn the values of involved parameters by minimizing the regularized squared error function associated with (\ref{eq:svd++_min}): 
\begin{eqnarray}
\label{eq:svd++_min}
\begin{aligned}
mi{n_{{q_*},{p_*},\;\;{b_*},{y_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in K} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i} + \;q_i^T\;\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j  \in {N_u}} {{y_j}} } \right)} \right)^2} \\+ \;\lambda \left( {{b_u}^2 + \;{b_i}^2 + \;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2} + \sum\limits_{j \in {N_u}} {{{\left\| {{y_j}} \right\|}^2}} } \right)
\end{aligned}
\end{eqnarray}
Model parameters are determined by minimizing the associated regularized
squared error function through stochastic gradient descent. We loop over all training set, computing:

$
\begin{array}{l}{b_u} \leftarrow {b_u} + {\rm{\;}}{\gamma}.\left( {{e_{u,i}} - {\rm{\;}}{\lambda _1}.{b_u}} \right)\\{b_i} \leftarrow {b_i} + {\rm{\;}}{\gamma}.\left( {{e_{u,i}} - {\rm{\;}}{\lambda _1}.{b_i}} \right)\\{q_i} \leftarrow {q_i} + {\rm{\;}}{\gamma}.\left( {{e_{u,i}}.\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in {N_u}} {{y_j}} } \right) - {\lambda _2}.{q_i}} \right)\\{p_u} \leftarrow {p_u} + {\rm{\;}}{\gamma}.\left( {{e_{u,i}}.{q_i} - {\lambda _2}.{p_u}} \right)\\\forall j \in {N_u}:\\{y_j} \leftarrow {y_j} + {\rm{\;}}{\gamma}.\left( {{e_{ui}}.{{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}.{q_i} - {\lambda _2}.{y_j}} \right)\end{array}
$
\\\\
The SVD++ algorithm is summarized in the following pseudo-code:
\\

\begin{algorithm}[H]
 \KwData{$K$}
 \KwResult{Updated model }
 Initialize model\;
 \Repeat{convergence}{

	\ForAll{$(u, i, r) \in K$}
	{ 	${e_{u,i}} = \;{r_{u,i}} - \hat r_{u,i}$\\
		$\Delta b_u = e_{u,i} - \lambda_1 .{b_u}$\\
		$\Delta b_i = e_{u,i} - \lambda_1 .{b_i}$\\
		$\Delta p_u = e_{u,i}.q_i - \lambda_2 .{p_u}$\\
		$\Delta {q_i} \leftarrow {e_{ui}}.\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in  \in {N_u}} {{y_j}} } \right) - {\lambda _2}.{q_i}$\\ 
		
			$b_u = b_u + \gamma.\Delta b_u$\\
			$p_u = p_u + \gamma.\Delta b_u$\\
			\ForAll{$j \in N_u$}
			{
					${y_j} = {y_j} + {\rm{\;}}{\gamma}.\left( {{e_{ui}}.{{\left| {{N_u}} 									\right|}^{ - \frac{1}{2}}}.{q_i} - {\lambda _2}.{y_j}} \right)$
			}
			$b_i = b_i + \gamma.\Delta b_i$\\
			$q_i = q_i + \gamma.\Delta q_i$\\
	}
}
 \caption{SVD++}
\end{algorithm}
\clearpage

Several types of implicit feedback can be simultaneously introduced into the
model by using extra sets of item factors. For example, if a user $u$ has a certain
kind of implicit preference to the items in ${N^1}_u$ (e.g., she rented them), and a different
type of implicit feedback to the items in ${N^2}_u$ (e.g., she browsed them), we
could use the model: 
\begin{eqnarray}
\label{eq:svd++_pre2}
{r_{u,i}} = \;\mu  + \;{b_i} + \;{b_i} + q_i^T\left( {{p_u} + {{\left| {{N^1}_u} \right|}^{ - \frac{1}{2}}}\sum\limits_{j  \in {N^1}_u} {{y_j}}  + {{\left| {{N^2}_u} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in   {N^2}_u} {{y_j}} } \right)
\end{eqnarray}
The relative importance of each source of implicit feedback will be automatically
learned by the algorithm by its setting of the respective values of model parameters.
\section{Incremental SVD++}
Most users’ preferences are more or less consistent throughout a short period of time. In other words, if a user enjoys a certain type of music today, we assume that she will enjoy the same genre in the near future. This assumption is needed so that the collected past information can be utilized to produce recommendations for the future. If all users change their preferences, there is no way for a recommender
system to make any meaningful recommendations. In the other hand, the consistency is assumed for a short period of time. In real life scenarios, users do change preferences throughout time. The change might due to the awareness of new things, change of attitude, etc. So we must keep the recommender system up to date.

Figure~\ref{fig:RS_process} shows the process of a recommender system which can solve the above problem. This recommender system are trained using both online and offline training algorithms.
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"RS process".png} 
    \caption{The process of a recommender system}
    \!\!\!\!
    \caption*{Source: The Rapid-I Marketplace .}
    \label{fig:RS_process}
\end{figure}


The main task of SVD++ model is creating the features of users and items by minimizing the associated regularized squared error function through stochastic gradient descent. So it must loop for all known rating to get the best fit model. This becomes a big problem when applying the model in a large-scale dataset.
 
We make an assumption that is the new ratings do not effect the hold system. Fox example, when user Joe rates  a movie - Mummy. The features of Joe and Mummy change and need to be updated. But the features of another user Harry who has no relation to Joe or Mummy do not change. We propose Incremental SVD++, a new method based on the SVD++ technique. By cutting down the size of training data, our method reduces significant reduces the training time of classic SVD++, while maintaining its recommendation quality. 

\subsection{Notations}
We describe again some notations.
The set of all items that user $u$ have rated is denoted by $I_u$ and the set of all users who have rated item $i$ is denoted by $U_i$.
The set of all ratings user $u$ have rated is denoted by $R_{u,*}$ and the set of all ratings for items $i$ is denoted by $R_{*,i}$.
\subsection{Algorithm}
Similar to SVD++, the rating is predicted by the following equation:
\begin{eqnarray}
\label{eq:isvd++_pre1}
{\hat r_{u,i}} = \;\mu  + \;{b_i} + \;{b_i} + q_i^T\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in  \in {N_u}} {{y_j}} } \right)
\end{eqnarray}
Given that $r_{x,y}$ is the new rating given by user $x$ to item $y$.
In our method, we divide the set of known ratings $K$ into two subset: $S_1$ and $S_2$. $S_1$ includes all the ratings that have directly relation with $x$ or $y$ such as $R_{x,*}$ the known ratings given by $x$  and $R_{*,y}$ the known ratings for $y$. $S_2$ includes the rest known ratings which have no directly relation with $x$ or $y$.

For example, we have a ratings matrix:

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
         & A & B & C & D \\
        \hline
        $u_1$ & 5 & 3 &  & 1 \\
        \hline
        $u_2$ & 4 & 1 & \cellcolor{Gray}4 & 1 \\
        \hline
        $u_3$ & 1 &  &  & 5 \\
        \hline
        $u_4$ & 1 & 1 &  & 4 \\
        \hline
        $u_5$ &  &  & 5 & 4 \\
        \hline
    \end{tabular}
    \caption*{Ratings matrix $R_{m \times n}$, $m=5$, $n=4$}
\end{table}
User $u_2$ gives a new rating for $C$. So 

$R_{*,u_2} = \left\{r_{u_2A}, r_{u_2B}, r_{u_2C}, r_{u_2D}\right\}$

$R_{C,*} = \left\{r_{u_2C}, r_{u_5C}\right\}$

The set of related ratings: 

$S_1 = \left\{r_{u_2A}, r_{u_2B}, r_{u_2C}, r_{u_2D}, r_{u_5C}\right\}$
\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|g|c|}
        \hline
         & A & B & C & D \\
        \hline
        $u_1$ &  &  &  &  \\
        \hline
        \rowcolor{Gray}
        $u_2$ & 4 & 1 & 4 & 1 \\
        \hline
        $u_3$ &  &  &  &  \\
        \hline
        $u_4$ &  &  &  &  \\
        \hline
        $u_5$ &  &  & 5 &  \\
        \hline
    \end{tabular}
    \caption*{Related ratings matrix}
\end{table}  

The set of unrelated ratings: 

$S_2 = \left\{r_{u_1A}, r_{u_1B}, r_{u_1D}, r_{u_3A}, r_{u_3D}, r_{u_4A}, r_{u_4B}, r_{u_4D}, r_{u_5D}\right\}$

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|g|c|}
        \hline
         & A & B & C & D \\
        \hline
        $u_1$ & 5 & 3 &  & 1 \\
        \hline
        \rowcolor{Gray}
        $u_2$ &  &  &  &  \\
        \hline
        $u_3$ & 1 &  &  & 5 \\
        \hline
        $u_4$ & 1 & 1 &  & 4 \\
        \hline
        $u_5$ &  &  &  & 4 \\
        \hline
    \end{tabular}
    \caption*{ Unrelated ratings matrix}
\end{table}

The idea of using matrix factorization is build a features model that determine how a user rates an item. This feartures model contents features vectors of all users and items. And to learn this model, MF systems minimize the regularized squared error on the training set. In SVD and SVD++ approaches, training set is known ratings. Therefore, the model can explain all known ratings. With a new rating, the system updates the set of known rating and re-computes the model. 

In Incremental SVD++ method, we try to reduce the training set. 

We consider that: 
\begin{eqnarray}
\label{eq:isvd++_min}
\begin{aligned}
mi{n_{{q_*},{p_*},\;\;{b_*},{y_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in K} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i} + \;q_i^T\;\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j  \in {N_u}} {{y_j}} } \right)} \right)^2} \\[\jot]\\+ \;\lambda \left( {{b_u}^2 + \;{b_i}^2 + \;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2} + \sum\limits_{j \in {N_u}} {{{\left\| {{y_j}} \right\|}^2}} } \right)
\end{aligned}
\end{eqnarray}
is equevelant to the sum of (\ref{eq:isvd++_min1}) and (\ref{eq:isvd++_min2}) where $K= S_1 \cup S_2$, $S_1 \cap S_2 = \emptyset$, and the two components of the regularized squared error function are positive.
\begin{eqnarray}
\label{eq:isvd++_min1}
\begin{aligned}
mi{n_{{q_*},{p_*},\;\;{b_*},{y_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in S_1} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i} + \;q_i^T\;\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j  \in {N_u}} {{y_j}} } \right)} \right)^2} \\[\jot]\\+ \;\lambda \left( {{b_u}^2 + \;{b_i}^2 + \;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2} + \sum\limits_{j \in {N_u}} {{{\left\| {{y_j}} \right\|}^2}} } \right)
\end{aligned}
\end{eqnarray}

\begin{eqnarray}
\label{eq:isvd++_min2}
\begin{aligned}
mi{n_{{q_*},{p_*},\;\;{b_*},{y_*}}}\mathop \sum \limits_{\left( {u,\;i} \right) \in S_2} {\left( {{r_{u,i}} - \;\mu  + \;{b_u} + \;{b_i} + \;q_i^T\;\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j  \in {N_u}} {{y_j}} } \right)} \right)^2} \\[\jot]\\+ \;\lambda \left( {{b_u}^2 + \;{b_i}^2 + \;{{\left\| {{q_i}} \right\|}^2} + \;{{\left\| {{p_u}} \right\|}^2} + \sum\limits_{j \in {N_u}} {{{\left\| {{y_j}} \right\|}^2}} } \right)
\end{aligned}
\end{eqnarray}

To minimize the regularized squared error on the training set, we minimize the regularized squared error on each subset. The idea of Incremental SVD++ is new ratings only affect the features of related users and items. We assume that new rating $r_{x,y}$ only affect the features of user $x$ and item $y$. All the factors vectors and bias parameters of others users and others items unchanged.  

Model parameters are determined by minimizing the associated regularized squared error function through conditional gradient descent on the related ratings $S_1$. Obviously, \ref{eq:isvd++_min2} is unchanged, so it is not concerned in the learning process. 

$Q$ is denoted as the set of objects (e.g. $x$, $y$) which need to be updated. The Incremental SVD++ is summarized in the following pseudo-code:
\\\\

\begin{algorithm}[H]
 \KwData{$S_1$, $Q$}
 \KwResult{Updated model }
 Load model\;
 \Repeat{convergence}{

	\ForAll{$(u, i, r) \in S_1$}
	{ 	${e_{u,i}} = \;{r_{u,i}} - \hat r_{u,i}$\\
		$\Delta b_u = e_{u,i} - \lambda_1 .{b_u}$\\
		$\Delta b_i = e_{u,i} - \lambda_1 .{b_i}$\\
		$\Delta p_u = e_{u,i}.q_i - \lambda_2 .{p_u}$\\
		$\Delta {q_i} \leftarrow {e_{ui}}.\left( {{p_u} + {{\left| {{N_u}} \right|}^{ - \frac{1}{2}}}\sum\limits_{j \in  \in {N_u}} {{y_j}} } \right) - {\lambda _2}.{q_i}$\\ 
		\CommentSty{//this is an extra condition in Incremental SVD++}\\
		\If{$u \in Q$} 
		{
			$b_u = b_u + \gamma.\Delta b_u$\\
			$p_u = p_u + \gamma.\Delta b_u$\\
			\ForAll{$j \in N_u$}
			{
				\If{$j \in Q$}
				{
					
					${y_j} = {y_j} + {\rm{\;}}{\gamma}.\left( {{e_{ui}}.{{\left| {{N_u}} 									\right|}^{ - \frac{1}{2}}}.{q_i} - {\lambda _2}.{y_j}} \right)$
				}
			}
		}
		\CommentSty{//this is an extra condition in Incremental SVD++}\\
		\If{$i \in Q$} 
		{
			$b_i = b_i + \gamma.\Delta b_i$\\
			$q_i = q_i + \gamma.\Delta q_i$\\
		}
	}
}
 \caption{Incremental SVD++}
\end{algorithm}


\chapter{Experiments}
\label{experiments}

In this section, we conduct experiments to compare the performance
of our Incremental SVD++ algorithm with classic SVD++ algorithm.

\section{Experiment setup}
\subsection{Data}
We choose MovieLens\footnote{http://grouplens.org/datasets/movielens/} dataset to study empirical
performance of our algorithms. The basic statistics of the dataset.

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|}
        \hline
         & MovieLens  \\
        \hline
        No. ratings & 100 000 \\
        \hline
         No. ratings of training set & 90 000 \\
        \hline
         No. ratings of test set & 10 000 \\
        \hline
        No. users &  5949  \\
        \hline
        No. items &  3295 \\
        \hline
        Rating range & $\left[1, 5\right[$ \\
        \hline
        Rating type &  Integer\\
        \hline
    \end{tabular}
    \label{tab:MovieLens_datatest}
    \caption{ Statistics of MovieLens dataset}
\end{table}

The data set is  randomly from the MovieLens 1M dataset. All the ratings are ordered by the time stamp. Test set includes $10000$ newest ratings. Training set includes $90000$ ratings.
\subsection{Parameters} 
The meta-parameters are the same in all experiments. 
Number of factors $k = 100$. $\gamma = \lambda = 0.01$.

\section{Evaluation measures}
We adopt Root Mean Square Error(RMSE) to evaluate two algorithms i.e. SVD++ and Incremental SVD++.
RMSE evaluates the root of average square error between true rating and predicted rating. Denote the test set by $T$ , the definition of RMSE is given below:
\begin{eqnarray}
\label{eq:RMSE}
RMSE = \sqrt {\frac{{\sum\limits_{(u,i,r) \in T} {{{(\widehat {{r_{u,i}}} - r)}^2}} }}{{\left| T \right|}}} 
 \end{eqnarray}

\section{Experiment Result}
We denote the training set of the initialization model by $T$ and the updating set by $U$.  

\subsection{Comparisons between SVD++ and Incremental SVD++}
The aim of this experiment is to compare the accuracy and training time of SVD++ and Incremental SDV++ in dynamic scenario where ratings happen continuously. 

Experimental 1 scenario: 
\begin{description}
    \item{\textbf{1.}} The model is pre-buit with a subset of $20000$ ratings.
    \item{\textbf{2.}} The ISVD++, as an online training system, updates the model per $2000$ new ratings.
    \item{\textbf{3.}} The SVD++, as an offline training system, updates the model per $10000$ new ratings.
\end{description}



%\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{"result1".png} 
    \caption{ Recommendation accuracy (RMSE) comparison between the proposed Incremental SVD++ and SVD++ algorithm in MovieLens}
    \label{fig:RMSE}
\end{figure}
Figure~\ref{fig:RMSE} shows a comparison between SVD++ and ISVD++ based on their recommendation accuracies. SVD++ methods updates the model after each $10000$ new ratings. So the recommendations are not modified in the period between two updates and the accuracy remained the same. On the other hand, ISVD++ constantly updates the model after each $2000$ news ratings. According to the increasing of observed ratings, the accuracy of two methods significantly increases. But ISVD++ has a slightly better accuracy.

\clearpage

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{"training_timeSVD".png} 
    \caption{Training times (ms) of SVD++}
    \label{fig:train_timeSVD++}
\end{figure}
%\clearpage

Average training time of SVD++: $\bar t_{SVD++} = 305832.75$

Figure~\ref{fig:train_timeSVD++} shows the traning times of SVD++ method. It is clearly seen that the training times of SVD++ method increases rapidly according to the number of observed ratings. This is a drawback of SVD++ method when applying in a large-scale dataset. 
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{"training_timeISVD".png} 
    \caption{Updating times (ms) of ISVD++}
    \label{fig:train_timeISVD++}
\end{figure}
%\clearpage
Average training time of ISVD++: $\bar t_{ISVD++} =94476.44$

The updating times of ISVD++ method are erratic. Unlike SVD++, the updating time of ISVD++ does not depend on the number of observed ratings. Because ISVD++ only scans over a subset of ratings when modifying the model.
In the next experiments, we will evaluate the factors that affect the updating tims of ISVD++ method. Based on the average updating time, ISVD++ method has smaller updating time than than SVD++ method.

Based on the accuracy (RMSE) and the updating time, ISVD++ performs better recommendation than SVD++.
\subsection{Impact of Parameters}
\label{Impact of Parameters}
In this section, we evaluate the impact of parameters on the performance of ISVD++.
\subsubsection{Impact of the stable of pre-built model}
We assume that the stable of pre-built model depends on the number of observed ratings which are used to build the model. In this experiment, we evaluate the impact of the number of ratings which are used to pre-built the model, on the updating time of ISVD++ method.

Experimental 2 scenario: 
\begin{quote}
The set of new ratings is fixed. ISVD++ will integrate this dataset into models which built of various training datasets. The number of ratings in each dataset are different. 

Specifically, the number of ratings in each training datasets are: $20000$, $30000$, $40000$, $50000$, $60000$, $70000$, $80000$. The updating data includes $2000$ ratings.
\end{quote}
\clearpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{"init".png} 
    \caption{Updating times (ms) of ISVD++}
    \label{fig:train_timeISVD++_impact_1}
\end{figure}
Figure~\ref{fig:train_timeISVD++_impact_1} shows the updating times of ISVD++ method with the given trainning datasets. The updating times of ISVD++ method increases steadily according to the number of new ratings.

\subsubsection{Impact of the number of new ratings}
As mentioned before, ISVD++ method only scans over a subset of ratings which is relevant to new ratings. So in this experiment, we evaluate the relation between the updating time of ISVD++ method and the number of updating dataset.
 
Experimental 3 scenario: 
\begin{quote}
The model is pre-buit with a fix number of ratings. ISVD++ updates the model with various dataset. And the number of ratings in these dataset are different. Each update, the model will be rebuilt based on the original dataset. So the stable of pre-built model does not affect the result. 

Specifically, the training data includes $70000$ ratings. The number of ratings in updating datasets are: $1$, $100$, $1000$, $5000$, $10000$, $20000$.
\end{quote}


\begin{figure}[h!]
    \centering
    \includegraphics[width=0.9\textwidth]{"no_new".png} 
    \caption{Updating times (ms) of ISVD++}
    \label{fig:train_timeISVD++_impact_2}
\end{figure}
%\clearpage
Figure~\ref{fig:train_timeISVD++_impact_2} shows the updating times of ISVD++ method with the given updating datasets. It is clearly seen that the updating times of ISVD++ method increases rapidly according to the number of new ratings. It cost too much time to update model with large updating data.

\subsubsection{Explanation}
The result in experiment 2 and experiment 3 can be explained by the number of related ratings in each experiment. Table~\ref{tab:related_1} and ~\ref{tab:related_2} show the number of related ratings in each experiment.
\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        No. new ratings & 1 &	100 &	1000&	5000&	10000&	20000\\
        \hline
         No. related ratings & 47&	5650&	33003&	60865&	66246&	68502 \\
        \hline
    \end{tabular}
    \caption{ The number of related ratings in experiment 2}
    \label{tab:related_1}
\end{table}

\begin{table}[h!]
    \small\centering
    \begin{tabular}{|c|c|c|c|c|c||c|c|}
        \hline
        No. traning ratings & 20000	&30000	&40000	&50000&	60000&	70000&	80000\\
        \hline
         No. related ratings & 15126&	20475&	27147&	33984&	40887&	47873&	54736\\
        \hline
    \end{tabular}
    
    \caption{ The number of related ratings in experiment 3}
    \label{tab:related_2}
\end{table}

The updating step of ISVD++ is faster than SVD++ because it only minimizes the regularized squared error on a subset, the related ratings $S_1$. In experiment 2 and experiment 3, the number of related ratings changes when adjusting other parameters. Therefore, the updating time changes according to the related dataset. 

On the other hand, the increasing of the number of related ratings reduces the speed of the system. In order to get the best performance of ISVD++, the size of updating steps must be adjusting based on the related ratings.

\chapter{Summarization}
\label{Summarization}
\section{Summarization}
We have thoroughly investigated the 
Incremental SVD++ has performed an out standing performance in comparison with one of the best recommendation algorithm in the world - Singular Value Decomposition with implicit feedback.

In a dynamic scenario, Incremental SVD++ totally beats SVD++ in updating time. In average, ISVD++ run faster than SVD++ 3 times. ISVD++ also produces a slightly better recommendation accuracy then SVD++.

On the other hand, we also evaluate the impart of the size of training data and updating data on the updating time of ISVD++. In order to apply ISVD++ into a real system, we have to adjust the size of updating step to achieve the best results.
 
\section{Future work}
\label{future_work}
Incremetal SVD++ has a major flaw that is it can not run as an offline algorithms. If we use ISVD++ to pre-compute the model, the systems will face the same  as using ISVD++ to update model with a huge updating dataset.  So in order to get advantage of live update from incremetal SVD++ and recommendations for new data we propose a system where both SVD++ and Incremental SVD++ must be implemented.

Also despite the fact that Incremental SVD++ was researched for scaled commercial systems, the project was not production ready. Every time we want to recommend a list of items to user we need to reload the whole trained model into memory in which when the model file size becomes to big it won't be able to fit onto the computer's RAM.

\subsection{Application using Incremental SVD++ with offline SVD++}

In this project we computed and stored the trained model into our data structure matrices - sparse matrix and dense matrix - which the size of the matrices were defined before hand by processing the input data such as the number of items, the number of users and the number of implicit feedback features for each user. These matrices are isolated within the input data, new users and items with new ratings won't get their recommendation list until the whole data model is re-trained, so a system with Incremental SVD++ alone won't generate much profit. In this thesis, we propose a system using the combination of both SVD++ and Incremental SVD++ where Incremental SVD++ is call periodically to update ratings for old data (e.g. update every several minutes) and daily rebuild new model with new data using SVD++.

\subsection{Storing model to database management system}

Our model data is first stored in matrices and then writed to binary files. Next time we want to update our model or suggest items to someone, we have to load the model from files to our data struture and store it in the memory. The problem for scale is that, when the files size exceed the memory size, the memory will be out of ram and crash the application.

On the other hand, our Incremental SVD++ algorithm was fast because it only rebuild a portion of the trained model to predict items more up to date. But with the currently save-load model mechanism, time and resource was wasted to load and save other unnecessary data during the process.

To be able to scale and using Incremental SVD++ more effectively, we suggest re-structuring data and storing SVD++ trained model into a database management system such as relational database like MySQL or Microsoft SQL Server. By using these relational database management system (RDBMS) we can take advantage of the built-in scalability options and responsive support from its' huge communities. We may also reduce a lot of load time since only relevant data was load.

Propose database structure: 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\textwidth]{"database".png} 
    \caption{Database structure}
    \label{fig:database}
\end{figure}
%\clearpage



\vfill
\noindent{\fbox{\parbox{\textwidth}{Last but not least, we will welcome anyone caring about this thesis to give ideas or join us and develop Incremental SVD++. Please contact us at: \textsf{cdanh@apcs.vn} (Dung-Anh CAO) or \textsf{hpanh@apcs.vn} (Phuong-Anh HOANG) for more information.}}}



\bibliographystyle{ieeetr}
\bibliography{reference}
\begin{thebibliography}{10}
%1
\bibitem{SarwarApplication}  
B.M. Sarwar, G. Karypis, J. A. Konstan, and J. Riedl, "Application of Dimensionality Reduction in Recommender System—A Case Study", WEBKDD’2000.

%2
\bibitem{SimonFunk}
S. Funk, "Netflix Update: Try This At Home," http://sifter.org/˜simon/journal/20061211.html, 2006.

%3
\bibitem{BellKorFactor}  
Y. Koren, “Factorization Meets the Neighborhood: a Multifaceted Collaborative Filtering
Model," \emph{Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining}, 2008.

%4
\bibitem{KorenMatrix} 
Y. Koren, “Matrix Factorization Techniques for Recommender Systems,” Published by the IEEE Computer Society, IEEE 0018-9162/09, pp. 42- 49, ©IEEE, August 2009.

%5
\bibitem{Hill} 
W. Hill, L. Stead, M. Rosenstein, and G. Furnas, “Recommending and evaluating choices in a virtual community of use,” In \emph{Proc. of the SIGCHIconference on Human factors in computing systems}, pages 194-201, 1995.

%6
\bibitem{Resnick} 
P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl, “GroupLens: An Open Architecture for Collaborative Filtering of Netnews,” In \emph{Proc. of the ACM Conf. on Computer Supported Cooperative Work}, pages 175-186, 1994.

%7
\bibitem{Shardanand} 
U. Shardanand and P. Maes, “Social information ltering: Algorithms
for automating word of mouth,” In \emph{Proc. of ACM Conf. on Human Factors in Computing Systems}, pages 210-217, 1995.

%8
\bibitem{Adomavicius} 
G. Adomavicius and A. Tuzhilin, “Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions,” \emph{IEEE Tran. on Knowledge and Data Engineering}, 17(6):734-749, 2005.

%9
\bibitem{Balabanovic} 
M. Balabanovic and Y. Shoham, “Fab: content-based, collaborative recommendation,” \emph{Comm},  ACM, 40(3):66-72, 1997.

%10
\bibitem{TranHybrib} 
T.Tran, and R.Cohen, “Hybrid recommender systems for electronic commerce,” In \emph{Proc. Knowledge-Based Electronic Markets},  Papers from the AAAI Workshop, Technical Report WS-00-04, AAAI Press.


%11
\bibitem{Chevalier} 
C. J. C. S.-D. Max Chevalier, \emph{Collaborative and Social Information Retrieval and Access: Techniques for Improved User Modeling}, Information Science Reference - Imprint of: IGI Publishing Hershey, PA, 2008. 

%12
\bibitem{Linden} 
G. Linden, B. Smith, J. York, "Amazon.com Recommendations: Item-to-Item
Collaborative Filtering," \emph{IEEE Internet Computing}, January 2003

%13
\bibitem{Lops11}
P.Lops, M.Gemmis, and G. Semeraro "Content-based recommender systems: State of the art and
trends," In Francesco Ricci, Lior Rokach, Bracha Shapira, and
Paul B. Kantor, editors, \emph{Recommender Systems Handbook},
pages 73–105. Springer US, 2011

%14
\bibitem{Resnick}
P. Resnick and H. R. Varian, “Recommender systems,” \emph{Commun. ACM}, vol. 40, pp. 56–58, March 1997.

%14
\bibitem{Oard98}
D.W. Oard and J. Kim, “Implicit Feedback for Recommender Systems”, \emph{Proc. 5th DELOS Workshop on Filtering and Collaborative Filtering}, pp. 31–36, 1998.

%15
\bibitem{Resnick94}
P. Resnick, N. Iacovou, M. Suchak, P. Bergstrom, and J. Riedl, "Grouplens: An open architecture for collaborative filtering of netnews," In \emph{CSCW}, pages 175-186, 1994.

%16
\bibitem{Sarwar01}
B. M. Sarwar, G. Karypis, J.A. Konstan, and J. Riedl. "Item-based collaborative filtering recommendation
algorithms," In \emph{WWW}, pages 285–295, 2001.

%16
\bibitem{Burke07}
R. Burke,"Hybrid Web Recommender Systems," \emph{The Adaptive Web,
Methods and Strategies of Web Personalization}, LNCS 4321, 2007

%17
\bibitem{Burke02}
R. Burke, "Hybrid Recommender Systems: Survey and Experiments," \emph{User Modeling and User-Adapted Interaction}, 12(4):331–370, 2002

%18
\bibitem{Salakhutdinov07}
R. Salakhutdinov, A. Mnih, and G.E. Hinton, "Restricted boltzmann machines for collaborative
filtering," In \emph{ICML}, pages 791–798, 2007.

%19
\bibitem{SalakhutdinovMF07}
R. Salakhutdinov and A. Mnih, "Probabilistic matrix factorization," In \emph{NIPS}, 2007.

%20
\bibitem{Zhang10}
Y. Zhang, B. Cao, and D.Y. Yeung, "Multidomain collaborative filtering," In \emph{UAI}, pages 725–732,
2010.

%21
\bibitem{Jin03}
R. Jin, L. Si, and C. Zhai. Preferencebased graphic models for collaborative filtering. In
\emph{UAI}, pages 329–336, 2003.

%22
\bibitem{Rendle10}
S. Rendle and L. Schmidt-Thieme, "Pairwise interaction tensor factorization for personalized tag
recommendation," In \emph{WSDM}, pages 81–90, 2010.

%23
\bibitem{Ma11}
H. Ma, D. Zhou, C. Liu, M.R. Lyu, and I. King, "Recommender systems with social
regularization," In \emph{WSDM}, pages 287–296, 2011.

\end{thebibliography}


%\chapter*{Appendix}
%\label{appendix}
%\addcontentsline{toc}{chapter}{Appendix}

\printindex

\end{document}
